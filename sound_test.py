import torch
import torch.nn as nn
import librosa
import numpy as np
import pyaudio
import keyboard
import time

# è®¾å¤‡é€‰æ‹©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ¦ ç±»åˆ«è®¾ç½®
num_classes = 3
label_map = {0: 'crow', 1: 'magpie', 2: 'sparrow'}

# ğŸµ æ¨¡å‹ç»“æ„ï¼ˆå’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(32 * 10 * 54, 128), nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)

# ğŸ“¦ åŠ è½½æ¨¡å‹
model = SimpleCNN(num_classes).to(device)
model.load_state_dict(torch.load('audio_model.pth', map_location=device))
model.eval()

# éŸ³é¢‘å‚æ•°
sample_rate = 22050
duration = 5
n_mfcc = 40
threshold = 0.5  # æœ€ä½ç½®ä¿¡åº¦

# å®æ—¶å½•éŸ³è®¾ç½®
p = pyaudio.PyAudio()
chunk = 1024
channels = 1
format = pyaudio.paInt16
rate = sample_rate

def listen_and_recognize():
    stream = p.open(format=format,
                    channels=channels,
                    rate=rate,
                    input=True,
                    frames_per_buffer=chunk)

    print("ğŸ§ è¯†åˆ«æ¨¡å¼å¼€å¯ï¼ŒæŒ‰ 'q' é”®é€€å‡º")
    while True:
        if keyboard.is_pressed('q'):
            print("âŒ é€€å‡ºè¯†åˆ«æ¨¡å¼")
            break

        print("ğŸ”´ æ­£åœ¨å½•éŸ³...")
        audio_data = []
        for _ in range(0, int(sample_rate / chunk * duration)):
            data = stream.read(chunk)
            audio_data.append(np.frombuffer(data, dtype=np.int16))
        audio_data = np.hstack(audio_data)

        # ğŸ¼ ç‰¹å¾æå–
        mfcc = librosa.feature.mfcc(y=audio_data.astype(float), sr=sample_rate, n_mfcc=n_mfcc)
        if mfcc.shape[1] < 216:
            pad_width = 216 - mfcc.shape[1]
            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
        else:
            mfcc = mfcc[:, :216]

        mfcc_tensor = torch.tensor(mfcc).float().unsqueeze(0).unsqueeze(0).to(device)

        # ğŸ” æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            output = model(mfcc_tensor)
            prob = torch.softmax(output, dim=1)
            confidence, predicted_class = torch.max(prob, 1)

        # ğŸ“¢ è¾“å‡ºç»“æœ
        if confidence.item() > threshold:
            bird_name = label_map[predicted_class.item()]
            print(f"âœ… æ£€æµ‹ç»“æœ: {bird_name}ï¼ˆç½®ä¿¡åº¦ {confidence.item():.2f}ï¼‰")
        else:
            print("âš ï¸ ç½®ä¿¡åº¦ä½ï¼Œæ— æ³•åˆ¤æ–­é¸Ÿç±»")

        time.sleep(1)  # é—´éš”ä¸€ç§’å†æ¬¡è¯†åˆ«

    stream.stop_stream()
    stream.close()

# ğŸ•¹ï¸ ä¸»ç¨‹åºï¼šæŒ‰é”®å¯åŠ¨è¯†åˆ«
def start_recognition():
    print("ğŸ“¢ æŒ‰ 's' å¼€å§‹è¯†åˆ«ï¼ŒæŒ‰ 'q' é€€å‡ºç¨‹åº")
    while True:
        if keyboard.is_pressed('s'):
            listen_and_recognize()
        if keyboard.is_pressed('q'):
            print("ğŸšª é€€å‡ºç¨‹åº")
            break

start_recognition()
